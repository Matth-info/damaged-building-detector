{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by : How to Use Deep Learning, PyTorch Lightning, and the Planetary Computer to Predict Cloud Cover in Satellite Imagery (https://drivendata.co/blog/cloud-cover-benchmark/) \\\n",
    "Data Source : https://source.coop/repositories/radiantearth/cloud-cover-detection-challenge/ \\\n",
    "A complete access to DrivenData Cloud Cover Detection Challenge through Kaggle API : https://www.kaggle.com/datasets/hmendonca/cloud-cover-detection/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloud-cover-detection-challenge/final/private/test_labels/aaaa.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aaay.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aabb.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aabd.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aabl.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aaci.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aacl.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aacm.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aaco.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aacy.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aaeg.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aaen.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aaha.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aahh.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aaii.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aaiq.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aajr.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aajt.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aakj.tif\n",
      "cloud-cover-detection-challenge/final/private/test_labels/aakv.tif\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "bucket_name = 'radiantearth'\n",
    "online_folder='test_labels'  # 'test_features' 'test_labels' 'train_features'  #'train_labels' # 'train_features' \n",
    "state = 'private' #'public' # 'private'\n",
    "prefix = f'cloud-cover-detection-challenge/final/{state}/{online_folder}'\n",
    "local_dir = f'../data/cloud_data/final/{state}/{online_folder}'\n",
    "\n",
    "# required to add to the environment variable : AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\n",
    "s3 = boto3.client('s3', endpoint_url='https://data.source.coop')\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "for obj in response.get('Contents', []):\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure the local directory exists\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# List and download files\n",
    "objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "for obj in objects.get('Contents', []):\n",
    "    file_key = obj['Key']\n",
    "    local_file_path = os.path.join(local_dir, os.path.relpath(file_key, prefix))\n",
    "    \n",
    "    # Create any necessary subdirectories\n",
    "    os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "    \n",
    "    # Download file\n",
    "    s3.download_file(bucket_name, file_key, local_file_path)\n",
    "    print(f\"Downloaded {file_key} to {local_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data using Kaggle API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run : pip install kaggle \n",
    "- Connect to kaggle API (username and API Key via Environmental variable) : export KAGGLE_USERNAME=... and export KAGGLE_KEY=...\n",
    "- run : kaggle datasets download -d hmendonca/cloud-cover-detection (27GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from S3 bucket "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloud data \n",
    "    final   \n",
    "        private\n",
    "            test_features\n",
    "            test_labels\n",
    "        public\n",
    "            train_features\n",
    "            train_labels\n",
    "            train_metadata.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_path import path\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path.cwd().parent.resolve() / \"data/cloud_data/final/public\"\n",
    "TRAIN_FEATURES = DATA_DIR / \"train_features\"\n",
    "TRAIN_LABELS = DATA_DIR / \"train_labels\"\n",
    "\n",
    "assert TRAIN_FEATURES.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDS = [\"B02\", \"B03\", \"B04\", \"B08\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cloudpath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adwp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adwu</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adwz</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adxp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adxp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeaj</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/aeaj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chip_id  location              datetime                   cloudpath\n",
       "0    adwp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwp\n",
       "1    adwu  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwu\n",
       "2    adwz  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwz\n",
       "3    adxp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adxp\n",
       "4    aeaj  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/aeaj"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta = pd.read_csv(DATA_DIR / \"train_metadata.csv\")\n",
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature directory : /home/onyxia/work/damaged-building-detector/data/cloud_data/final/public/train_features\n",
      "Label directory : /home/onyxia/work/damaged-building-detector/data/cloud_data/final/public/train_labels\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cloudpath</th>\n",
       "      <th>B02_path</th>\n",
       "      <th>B03_path</th>\n",
       "      <th>B04_path</th>\n",
       "      <th>B08_path</th>\n",
       "      <th>label_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adwp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwp</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adwu</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwu</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adwz</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwz</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adxp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adxp</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeaj</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/aeaj</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chip_id  location              datetime                   cloudpath  \\\n",
       "0    adwp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwp   \n",
       "1    adwu  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwu   \n",
       "2    adwz  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwz   \n",
       "3    adxp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adxp   \n",
       "4    aeaj  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/aeaj   \n",
       "\n",
       "                                            B02_path  \\\n",
       "0  /home/onyxia/work/damaged-building-detector/da...   \n",
       "1  /home/onyxia/work/damaged-building-detector/da...   \n",
       "2  /home/onyxia/work/damaged-building-detector/da...   \n",
       "3  /home/onyxia/work/damaged-building-detector/da...   \n",
       "4  /home/onyxia/work/damaged-building-detector/da...   \n",
       "\n",
       "                                            B03_path  \\\n",
       "0  /home/onyxia/work/damaged-building-detector/da...   \n",
       "1  /home/onyxia/work/damaged-building-detector/da...   \n",
       "2  /home/onyxia/work/damaged-building-detector/da...   \n",
       "3  /home/onyxia/work/damaged-building-detector/da...   \n",
       "4  /home/onyxia/work/damaged-building-detector/da...   \n",
       "\n",
       "                                            B04_path  \\\n",
       "0  /home/onyxia/work/damaged-building-detector/da...   \n",
       "1  /home/onyxia/work/damaged-building-detector/da...   \n",
       "2  /home/onyxia/work/damaged-building-detector/da...   \n",
       "3  /home/onyxia/work/damaged-building-detector/da...   \n",
       "4  /home/onyxia/work/damaged-building-detector/da...   \n",
       "\n",
       "                                            B08_path  \\\n",
       "0  /home/onyxia/work/damaged-building-detector/da...   \n",
       "1  /home/onyxia/work/damaged-building-detector/da...   \n",
       "2  /home/onyxia/work/damaged-building-detector/da...   \n",
       "3  /home/onyxia/work/damaged-building-detector/da...   \n",
       "4  /home/onyxia/work/damaged-building-detector/da...   \n",
       "\n",
       "                                          label_path  \n",
       "0  /home/onyxia/work/damaged-building-detector/da...  \n",
       "1  /home/onyxia/work/damaged-building-detector/da...  \n",
       "2  /home/onyxia/work/damaged-building-detector/da...  \n",
       "3  /home/onyxia/work/damaged-building-detector/da...  \n",
       "4  /home/onyxia/work/damaged-building-detector/da...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_paths(df, feature_dir, label_dir=None, bands=BANDS):\n",
    "    \"\"\"\n",
    "    Given dataframe with a column for chip_id, returns a dataframe with a column\n",
    "    added indicating the path to each band's TIF image as \"{band}_path\", eg \"B02_path\".\n",
    "    A column is also added to the dataframe with paths to the label TIF, if the\n",
    "    path to the labels directory is provided.\n",
    "    \"\"\"\n",
    "    for band in bands:\n",
    "        df[f\"{band}_path\"] = feature_dir / df[\"chip_id\"] / f\"{band}.tif\"\n",
    "        #assert df[f\"{band}_path\"].path.exists().all()\n",
    "    if label_dir is not None:\n",
    "        df[\"label_path\"] = label_dir / (df[\"chip_id\"] + \".tif\")\n",
    "        #assert df[\"label_path\"].path.exists().all()\n",
    "\n",
    "    return df\n",
    "\n",
    "print(f\"Feature directory : {TRAIN_FEATURES}\")\n",
    "print(f\"Label directory : {TRAIN_LABELS}\")\n",
    "train_meta = add_paths(train_meta, TRAIN_FEATURES, TRAIN_LABELS)\n",
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11748"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the bounding box to longitude and latitude \n",
    "import pyproj\n",
    "def lat_long_bounds(filepath):\n",
    "    \"\"\"Given the path to a GeoTIFF, returns the image bounds in latitude and\n",
    "    longitude coordinates.\n",
    "\n",
    "    Returns points as a tuple of (left, bottom, right, top)\n",
    "    \"\"\"\n",
    "    with rasterio.open(filepath) as im:\n",
    "        bounds = im.bounds\n",
    "        meta = im.meta\n",
    "    # create a converter starting with the current projection\n",
    "    current_crs = pyproj.CRS(meta[\"crs\"])\n",
    "    crs_transform = pyproj.Transformer.from_crs(current_crs, current_crs.geodetic_crs)\n",
    "\n",
    "    # returns left, bottom, right, top\n",
    "    return crs_transform.transform_bounds(*bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Color image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_channel_rasterio(filepath):\n",
    "    \"\"\"Load a single channel from a TIF file using rasterio.\"\"\"\n",
    "    with rasterio.open(filepath) as src:\n",
    "        return src.read(1)\n",
    "\n",
    "def load_channel_pil(filepath):\n",
    "        return np.array(Image.open(filepath))\n",
    "\n",
    "def true_color_img(chip_id, data_dir=TRAIN_FEATURES, load_channel_f=load_channel_pil):\n",
    "        # Open image files as arrays, optionally including NIR channel\n",
    "\n",
    "        chip_dir = data_dir / chip_id\n",
    "\n",
    "        raw_rgb = np.stack([load_channel_f(chip_dir / \"B04.tif\"),\n",
    "                            load_channel_f(chip_dir / \"B03.tif\"),\n",
    "                            load_channel_f(chip_dir / \"B02.tif\"),\n",
    "                           ], axis=-1)\n",
    "    \n",
    "        # Normalize pixel values 0-1 values\n",
    "        return raw_rgb / raw_rgb.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "def display_random_chip(metadata, data_dir=TRAIN_FEATURES):\n",
    "\n",
    "    list_images = os.listdir(path=data_dir)\n",
    "    random_chip_id = np.random.choice(list_images)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    random_chip = metadata[metadata[\"chip_id\"]==random_chip_id].iloc[0]\n",
    "    \n",
    "    ax[0].imshow(true_color_img(random_chip.chip_id))\n",
    "    ax[0].set_title(f\"Chip {random_chip.chip_id}\\n(Location: {random_chip.location})\")\n",
    "    label_im = Image.open(random_chip.label_path)\n",
    "    ax[1].imshow(label_im)\n",
    "    ax[1].set_title(f\"Chip {random_chip.chip_id} label\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3877, 9), (7871, 9))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(9)  # set a seed for reproducibility\n",
    "\n",
    "# put 1/3 of chips into the validation set\n",
    "chip_ids = train_meta.chip_id.unique().tolist()\n",
    "val_chip_ids = random.sample(chip_ids, round(len(chip_ids) * 0.33))\n",
    "\n",
    "val_mask = train_meta.chip_id.isin(val_chip_ids)\n",
    "val = train_meta[val_mask].copy().reset_index(drop=True)\n",
    "train = train_meta[~val_mask].copy().reset_index(drop=True)\n",
    "\n",
    "val.shape, train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cloudpath</th>\n",
       "      <th>B02_path</th>\n",
       "      <th>B03_path</th>\n",
       "      <th>B04_path</th>\n",
       "      <th>B08_path</th>\n",
       "      <th>label_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adwp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwp</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adwu</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwu</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adxp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adxp</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aeaj</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/aeaj</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeap</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/aeap</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "      <td>/home/onyxia/work/damaged-building-detector/da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chip_id  location              datetime                   cloudpath  \\\n",
       "0    adwp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwp   \n",
       "1    adwu  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwu   \n",
       "2    adxp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adxp   \n",
       "3    aeaj  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/aeaj   \n",
       "4    aeap  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/aeap   \n",
       "\n",
       "                                            B02_path  \\\n",
       "0  /home/onyxia/work/damaged-building-detector/da...   \n",
       "1  /home/onyxia/work/damaged-building-detector/da...   \n",
       "2  /home/onyxia/work/damaged-building-detector/da...   \n",
       "3  /home/onyxia/work/damaged-building-detector/da...   \n",
       "4  /home/onyxia/work/damaged-building-detector/da...   \n",
       "\n",
       "                                            B03_path  \\\n",
       "0  /home/onyxia/work/damaged-building-detector/da...   \n",
       "1  /home/onyxia/work/damaged-building-detector/da...   \n",
       "2  /home/onyxia/work/damaged-building-detector/da...   \n",
       "3  /home/onyxia/work/damaged-building-detector/da...   \n",
       "4  /home/onyxia/work/damaged-building-detector/da...   \n",
       "\n",
       "                                            B04_path  \\\n",
       "0  /home/onyxia/work/damaged-building-detector/da...   \n",
       "1  /home/onyxia/work/damaged-building-detector/da...   \n",
       "2  /home/onyxia/work/damaged-building-detector/da...   \n",
       "3  /home/onyxia/work/damaged-building-detector/da...   \n",
       "4  /home/onyxia/work/damaged-building-detector/da...   \n",
       "\n",
       "                                            B08_path  \\\n",
       "0  /home/onyxia/work/damaged-building-detector/da...   \n",
       "1  /home/onyxia/work/damaged-building-detector/da...   \n",
       "2  /home/onyxia/work/damaged-building-detector/da...   \n",
       "3  /home/onyxia/work/damaged-building-detector/da...   \n",
       "4  /home/onyxia/work/damaged-building-detector/da...   \n",
       "\n",
       "                                          label_path  \n",
       "0  /home/onyxia/work/damaged-building-detector/da...  \n",
       "1  /home/onyxia/work/damaged-building-detector/da...  \n",
       "2  /home/onyxia/work/damaged-building-detector/da...  \n",
       "3  /home/onyxia/work/damaged-building-detector/da...  \n",
       "4  /home/onyxia/work/damaged-building-detector/da...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features from labels\n",
    "feature_cols = [\"chip_id\"] + [f\"{band}_path\" for band in BANDS]\n",
    "\n",
    "val_x = val[feature_cols].copy()\n",
    "val_y = val[[\"chip_id\", \"label_path\"]].copy()\n",
    "\n",
    "train_x = train[feature_cols].copy()\n",
    "train_y = train[[\"chip_id\", \"label_path\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from typing import List, Optional\n",
    "\n",
    "class CloudDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_paths: pd.DataFrame,\n",
    "        bands: List[str],\n",
    "        y_paths: Optional[pd.DataFrame] = None,\n",
    "        transform: Optional[A.Compose] = None,\n",
    "        pytorch: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data = x_paths\n",
    "        self.label = y_paths\n",
    "        self.bands = bands\n",
    "        self.transform = transform\n",
    "        self.pytorch = pytorch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def load_channel_pil(self, filepath: str):\n",
    "        return np.array(Image.open(filepath))\n",
    "\n",
    "    def open_mask(self, filepath: str):\n",
    "        mask = self.load_channel_pil(filepath)\n",
    "        return np.where(mask == 255, 1, 0)  # Convert mask to binary\n",
    "\n",
    "    def open_as_array(self, idx: int, invert=False):\n",
    "        band_arrs = [self.load_channel_pil(self.data.loc[idx][f\"{band}_path\"]) for band in self.bands]\n",
    "        x_arr = np.stack(band_arrs, axis=-1)\n",
    "        if invert:\n",
    "            x_arr = x_arr.transpose((2, 0, 1))\n",
    "        x_arr /= x_arr.max()  # Normalize to [0,1]\n",
    "        \n",
    "        return x_arr\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        x = self.open_as_array(idx, invert=self.pytorch)\n",
    "        y = None \n",
    "        if self.label is not None:\n",
    "            y = self.open_mask(self.label.loc[idx, 'label_path'])\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=x, mask=y) if y is not None else self.transform(image=x)\n",
    "            x = augmented['image']\n",
    "            y = augmented.get('mask', y)\n",
    "\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "        return (x, y) if y is not None else x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        # Return a string representation of the dataset\n",
    "        s = 'Dataset class with {} files'.format(self.__len__())\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import lr_scheduler\n",
    "from torchsummary import summary\n",
    "\n",
    "# Vision-related imports\n",
    "from torchvision.models import resnet18, resnet34\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "class ResNet_UNET(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=2, pretrained=True, freeze_backbone = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Modify first layer of ResNet34 to accept custom number of channels\n",
    "        base_model = resnet18(weights=pretrained)  # Change this line\n",
    "        base_model.conv1 = torch.nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        self.base_layers = list(base_model.children())\n",
    "        self.freeze_backbone(freeze_backbone)\n",
    "\n",
    "        # Define the Unet Head/Neck\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        \n",
    "        self.upconv4 = self.expand_block(512, 256)\n",
    "        self.upconv3 = self.expand_block(256*2, 128)\n",
    "        self.upconv2 = self.expand_block(128*2, 64)\n",
    "        self.upconv1 = self.expand_block(64*2, 64)\n",
    "        self.upconv0 = self.expand_block(64*2, out_channels)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Contracting Path\n",
    "        layer0 = self.layer0(x)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        # Expansive Path\n",
    "        upconv4 = self.upconv4(layer4)\n",
    "        upconv3 = self.upconv3(torch.cat([upconv4, layer3], 1))\n",
    "        upconv2 = self.upconv2(torch.cat([upconv3, layer2], 1))\n",
    "        upconv1 = self.upconv1(torch.cat([upconv2, layer1], 1))\n",
    "        upconv0 = self.upconv0(torch.cat([upconv1, layer0], 1))\n",
    "\n",
    "        return upconv0\n",
    "\n",
    "    def expand_block(self, in_channels, out_channels):\n",
    "        expand = nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "        )\n",
    "        return expand\n",
    "    \n",
    "    def freeze_backbone(self, freeze_backbone):\n",
    "        if freeze_backbone:\n",
    "            for l in self.base_layers:\n",
    "                for param in l.parameters():\n",
    "                    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics \n",
    "class MeanPixelAccuracy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPixelAccuracy, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Calculate pixel-wise accuracy by comparing predicted and ground truth labels\n",
    "        correct_pixels = (pred.argmax(dim=1) == target.to(pred.device)).float()\n",
    "        mean_accuracy = correct_pixels.mean()\n",
    "        return mean_accuracy\n",
    "\n",
    "class IoUMetric:\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        self.smooth = smooth  # Smooth term to prevent division by zero\n",
    "\n",
    "    def __call__(self, pred, target):\n",
    "        # pred shape: (batch_size, num_classes, height, width)\n",
    "        # target shape: (batch_size, height, width)\n",
    "\n",
    "        # Apply argmax to get predicted class per pixel\n",
    "        pred = pred.argmax(dim=1)  # shape (batch_size, height, width)\n",
    "\n",
    "        # Flatten predictions and target for calculation\n",
    "        pred = pred.view(-1)       # shape (batch_size * height * width,)\n",
    "        target = target.view(-1)   # shape (batch_size * height * width,)\n",
    "\n",
    "        # Create a tensor to store IoU for each class\n",
    "        num_classes = pred.max().item() + 1  # Determine the number of classes from pred\n",
    "        ious = []\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            pred_cls = (pred == cls).float()  # 1 for pixels in the predicted class, else 0\n",
    "            target_cls = (target == cls).float()  # 1 for pixels in the target class, else 0\n",
    "\n",
    "            # Calculate intersection and union\n",
    "            intersection = (pred_cls * target_cls).sum()\n",
    "            union = pred_cls.sum() + target_cls.sum() - intersection\n",
    "\n",
    "            # Compute IoU and add to the list\n",
    "            iou = (intersection + self.smooth) / (union + self.smooth)\n",
    "            ious.append(iou)\n",
    "\n",
    "        # Calculate mean IoU over all classes\n",
    "        mean_iou = torch.stack(ious).mean()\n",
    "        \n",
    "        return mean_iou\n",
    "\n",
    "\n",
    "class DiceMetric:\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        self.smooth = smooth  # Smooth term to prevent division by zero\n",
    "\n",
    "    def __call__(self, pred, target):\n",
    "        # pred shape: (batch_size, num_classes, height, width)\n",
    "        # target shape: (batch_size, height, width)\n",
    "\n",
    "        # Apply softmax to get probabilities for each class\n",
    "        pred = pred.softmax(dim=1)  # shape (batch_size, num_classes, height, width)\n",
    "        \n",
    "        # Flatten predictions and target across spatial dimensions\n",
    "        pred = pred.view(pred.size(0), pred.size(1), -1)  # (batch_size, num_classes, height * width)\n",
    "        target = target.view(target.size(0), 1, -1)       # (batch_size, 1, height * width)\n",
    "\n",
    "        # Create one-hot encoding for target\n",
    "        target_one_hot = torch.zeros_like(pred).scatter_(1, target, 1)  # (batch_size, num_classes, height * width)\n",
    "\n",
    "        # Calculate Dice score for each class\n",
    "        intersection = (pred * target_one_hot).sum(dim=2)  # sum over pixels\n",
    "        pred_sum = pred.sum(dim=2)\n",
    "        target_sum = target_one_hot.sum(dim=2)\n",
    "\n",
    "        # Compute Dice score for each class\n",
    "        dice_score = (2 * intersection + self.smooth) / (pred_sum + target_sum + self.smooth)\n",
    "        \n",
    "        # Average Dice score over classes\n",
    "        mean_dice = dice_score.mean()\n",
    "        \n",
    "        return mean_dice\n",
    "\n",
    "# Losses \n",
    "# Cross entropy loss : nn.CrossEntropyLoss()\n",
    "# Dice Loss \n",
    "# IoU loss \n",
    "\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(IoULoss, self).__init__()\n",
    "        self.smooth = smooth  # Smooth term to prevent division by zero\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # pred shape (batch_size, num_classes, height, width)\n",
    "        # target shape (batch_size, height, width)\n",
    "        \n",
    "        # Apply softmax to get probabilities for each class\n",
    "        pred = pred.softmax(dim=1)  # shape (batch_size, num_classes, height, width)\n",
    "        \n",
    "        # Flatten predictions and target across spatial dimensions\n",
    "        pred = pred.view(pred.size(0), pred.size(1), -1)  # shape (batch_size, num_classes, height * width)\n",
    "        target = target.view(target.size(0), 1, -1).long()  # shape (batch_size, 1, height * width)\n",
    "\n",
    "        # Create one-hot encoding for target\n",
    "        target_one_hot = torch.zeros_like(pred).scatter_(1, target, 1)  # shape (batch_size, num_classes, height * width)\n",
    "\n",
    "        # Calculate intersection and union\n",
    "        intersection = (pred * target_one_hot).sum(dim=2)  # sum over pixels\n",
    "        union = pred.sum(dim=2) + target_one_hot.sum(dim=2) - intersection  # sum over pixels\n",
    "        \n",
    "        # Compute IoU for each class and batch, add smooth for numerical stability\n",
    "        iou = (intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        # Average IoU over classes and batch\n",
    "        iou_loss = 1 - iou.mean()\n",
    "        \n",
    "        return iou_loss\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth  # Smooth term to prevent division by zero\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # pred shape (batch_size, num_classes, height, width), target shape (batch_size, height, width)\n",
    "        # Apply softmax to pred if needed and flatten the tensors\n",
    "        pred = pred.softmax(dim=1)  # ensure pred is a probability distribution per class\n",
    "        \n",
    "        # Convert to binary mask for Dice computation by treating each class separately\n",
    "        # Flatten across pixels and batch\n",
    "        pred = pred.view(pred.size(0), pred.size(1), -1)  # (batch_size, num_classes, height * width)\n",
    "        target = target.view(target.size(0), 1, -1).long()       # (batch_size, 1, height * width)\n",
    "        \n",
    "        # Create a one-hot target vector (batch_size, num_classes, height * width)\n",
    "        target_one_hot = torch.zeros_like(pred).scatter_(1, target, 1)\n",
    "\n",
    "        # Calculate Dice coefficient for each class\n",
    "        intersection = (pred * target_one_hot).sum(dim=2)\n",
    "        pred_sum = pred.sum(dim=2)\n",
    "        target_sum = target_one_hot.sum(dim=2)\n",
    "        \n",
    "        # Compute Dice loss\n",
    "        dice_score = (2 * intersection + self.smooth) / (pred_sum + target_sum + self.smooth)\n",
    "        dice_loss = 1 - dice_score.mean()  # Mean over classes and batch\n",
    "        \n",
    "        return dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model: nn.Module, dataloader: DataLoader, \n",
    "             loss_fns: List[nn.Module], loss_weights: List[float], \n",
    "             metric_fns: List[object], device: str, writer: SummaryWriter, epoch: int):\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    running_metrics = {metric.__class__.__name__: 0.0 for metric in metric_fns}\n",
    "    dict_losses = {loss_fn.__class__.__name__: [] for loss_fn in loss_fns}  # Dictionary to store individual losses\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (x, y) in enumerate(dataloader):\n",
    "            x, y = x.to(device), y.to(device).long()\n",
    "\n",
    "            total_loss = 0  # Reset loss for each batch\n",
    "            outputs = model(x)\n",
    "\n",
    "            # Calculate total loss with weights\n",
    "            for w, loss_fn in zip(loss_weights, loss_fns):\n",
    "                l = loss_fn(outputs, y)\n",
    "                total_loss += w * l\n",
    "                dict_losses[loss_fn.__class__.__name__].append(l.item())  # Store individual loss\n",
    "\n",
    "            # Calculate Metrics\n",
    "            for metric_fn in metric_fns:\n",
    "                metric_res = metric_fn(pred=outputs, target=y)\n",
    "                running_metrics[metric_fn.__class__.__name__] += metric_res.item() * dataloader.batch_size\n",
    "\n",
    "            running_loss += total_loss.item() * dataloader.batch_size\n",
    "\n",
    "            # Log individual loss to TensorBoard every 10 steps\n",
    "            if step % 10 == 0:\n",
    "                for loss_name, loss_values in dict_losses.items():\n",
    "                    writer.add_scalar(f'valid/{loss_name}_loss', loss_values[-1], epoch * len(dataloader) + step)\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    for metric_name in running_metrics:\n",
    "        running_metrics[metric_name] /= len(dataloader.dataset)\n",
    "\n",
    "    return epoch_loss, running_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "def train(model, train_dl, valid_dl, loss_fns, loss_weights=None, optimizer=None,\n",
    "          scheduler=None, metric_fns=None, epochs=1, experiment_name=\"experiment\",\n",
    "          log_dir=\"runs\", verbose=False, device=\"cuda:0\", model_dir=\"models\"):\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = os.path.join(log_dir, f\"{experiment_name}_{timestamp}\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    if loss_weights is None:\n",
    "        loss_weights = [1.0 / len(loss_fns)] * len(loss_fns)\n",
    "    elif abs(sum(loss_weights) - 1) > 1e-6:\n",
    "        raise ValueError(\"The loss weights must sum to 1!\")\n",
    "\n",
    "    model.to(device)\n",
    "    best_loss = float('inf')\n",
    "    train_loss, valid_loss = [], []\n",
    "    dict_losses = {loss_fn.__class__.__name__: [] for loss_fn in loss_fns}\n",
    "    overall_metrics = {metric.__class__.__name__: [] for metric in metric_fns or []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}\\n{\"-\" * 10}')\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for step, (x, y) in enumerate(train_dl):\n",
    "            x, y = x.to(device), y.to(device).long()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "\n",
    "            # Calculate weighted loss and track individual losses\n",
    "            total_loss = 0\n",
    "            for w, loss_fn in zip(loss_weights, loss_fns):\n",
    "                loss_val = loss_fn(outputs, y)\n",
    "                total_loss += w * loss_val\n",
    "                dict_losses[loss_fn.__class__.__name__].append(loss_val.item())  # Store individual loss\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += total_loss.item() * train_dl.batch_size\n",
    "\n",
    "            # Log individual loss to TensorBoard every few steps\n",
    "            if step % 10 == 0:\n",
    "                for loss_name, loss_values in dict_losses.items():\n",
    "                    writer.add_scalar(f'train/{loss_name}_loss', loss_values[-1], epoch * len(train_dl) + step)\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_dl.dataset)\n",
    "        train_loss.append(epoch_train_loss)\n",
    "        writer.add_scalar('train/epoch_loss', epoch_train_loss, epoch)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_metrics = validate(model, valid_dl, loss_fns, loss_weights, metric_fns, device)\n",
    "        valid_loss.append(val_loss)\n",
    "        writer.add_scalar('valid/loss', val_loss, epoch)\n",
    "        for metric_name, metric_value in val_metrics.items():\n",
    "            writer.add_scalar(f'valid/{metric_name}', metric_value, epoch)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            print(\"Updating best model\")\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(model_dir, f\"{experiment_name}_best_model.pth\"))\n",
    "\n",
    "        for metric_name, metric_value in val_metrics.items():\n",
    "            overall_metrics[metric_name].append(metric_value)\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "    writer.close()\n",
    "    print(f'Training complete in {(time.time() - start) // 60}m')\n",
    "    return train_loss, valid_loss, overall_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of augmentation pipeline\n",
    "transform = A.Compose([\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=45, p=0.5),\n",
    "        A.GridDistortion(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "        ToTensorV2()\n",
    "    ], additional_targets={'mask': 'mask'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CloudDataset(\n",
    "            x_paths=train_x,\n",
    "            bands=[\"B04\", \"B03\", \"B02\"],\n",
    "            y_paths=train_y,\n",
    "            transform=None\n",
    "        )\n",
    "valid_ds = CloudDataset(\n",
    "            x_paths=val_x,\n",
    "            bands=[\"B04\", \"B03\", \"B02\"],\n",
    "            y_paths=val_y,\n",
    "            transform=None\n",
    "        )\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be trained on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught UFuncTypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_25371/3703704132.py\", line 45, in __getitem__\n    x = self.open_as_array(idx, invert=self.pytorch)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_25371/3703704132.py\", line 40, in open_as_array\n    x_arr /= x_arr.max()  # Normalize to [0,1]\n    ^^^^^^^^^^^^^^^^^^^^\nnumpy._core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('uint16') with casting rule 'same_kind'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m exp_lr_scheduler \u001b[38;5;241m=\u001b[39m lr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer_ft, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m) \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m train_loss, valid_loss, overall_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_fns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_fns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../runs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mResNet_Unet_DrivenData\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dl, valid_dl, loss_fns, loss_weights, optimizer, scheduler, metric_fns, epochs, experiment_name, log_dir, verbose, device, model_dir)\u001b[0m\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     32\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 34\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1465\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/_utils.py:714\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    710\u001b[0m     exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexc_type(msg)\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught UFuncTypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_25371/3703704132.py\", line 45, in __getitem__\n    x = self.open_as_array(idx, invert=self.pytorch)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_25371/3703704132.py\", line 40, in open_as_array\n    x_arr /= x_arr.max()  # Normalize to [0,1]\n    ^^^^^^^^^^^^^^^^^^^^\nnumpy._core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('uint16') with casting rule 'same_kind'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Model will be trained on {device}\")\n",
    "\n",
    "model = ResNet_UNET(3, 2, pretrained=True, freeze_backbone=True).to(device)\n",
    "\n",
    "loss_fns = [DiceLoss(), nn.CrossEntropyLoss()]\n",
    "loss_weights = [0.4, 0.6]\n",
    "metric_fns = [IoUMetric(), MeanPixelAccuracy(), DiceMetric()]\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1) \n",
    "\n",
    "# Train the model\n",
    "train_loss, valid_loss, overall_metrics = train(\n",
    "    model, \n",
    "    train_dl,\n",
    "    valid_dl, \n",
    "    loss_fns=loss_fns,\n",
    "    loss_weights=loss_weights,\n",
    "    optimizer=optimizer_ft, \n",
    "    metric_fns=metric_fns,\n",
    "    log_dir=\"../runs\",\n",
    "    experiment_name='ResNet_Unet_DrivenData',\n",
    "    epochs=2,\n",
    "    device=device,\n",
    "    verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
