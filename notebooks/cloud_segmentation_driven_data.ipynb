{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Cover Segmentation Driven Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data from Source Coop API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by : How to Use Deep Learning, PyTorch Lightning, and the Planetary Computer to Predict Cloud Cover in Satellite Imagery (https://drivendata.co/blog/cloud-cover-benchmark/) \\\n",
    "Data Source : https://source.coop/repositories/radiantearth/cloud-cover-detection-challenge/ \\\n",
    "A complete access to DrivenData Cloud Cover Detection Challenge through Kaggle API : https://www.kaggle.com/datasets/hmendonca/cloud-cover-detection/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "bucket_name = 'radiantearth'\n",
    "online_folder='test_labels'  # 'test_features' 'test_labels' 'train_features'  #'train_labels' # 'train_features' \n",
    "state = 'private' #'public' # 'private'\n",
    "prefix = f'cloud-cover-detection-challenge/final/{state}/{online_folder}'\n",
    "local_dir = f'../data/cloud_data/final/{state}/{online_folder}'\n",
    "\n",
    "# required to add to the environment variable : AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\n",
    "s3 = boto3.client('s3', endpoint_url='https://data.source.coop')\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "for obj in response.get('Contents', []):\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure the local directory exists\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# List and download files\n",
    "objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "for obj in objects.get('Contents', []):\n",
    "    file_key = obj['Key']\n",
    "    local_file_path = os.path.join(local_dir, os.path.relpath(file_key, prefix))\n",
    "    \n",
    "    # Create any necessary subdirectories\n",
    "    os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "    \n",
    "    # Download file\n",
    "    s3.download_file(bucket_name, file_key, local_file_path)\n",
    "    print(f\"Downloaded {file_key} to {local_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data using Kaggle API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run : pip install kaggle \n",
    "- Connect to kaggle API (username and API Key via Environmental variable) : export KAGGLE_USERNAME=... and export KAGGLE_KEY=...\n",
    "- run : kaggle datasets download -d hmendonca/cloud-cover-detection (27GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data from S3 bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! mc cp s3/mbesnier/diffusion/damage_detection/cloud-segmentation-data/final/public/ ../data/Cloud_Driven/final/public"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a Jupyter notebook or IPython environment, run this in the first cell\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import prepare_cloud_segmentation_data, Cloud_DrivenData_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../data/Cloud_DrivenData/final/public\"\n",
    "train_share = 0.7\n",
    "train_x, train_y, val_x, val_y = prepare_cloud_segmentation_data(folder_path, train_share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "training_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(512, 512),\n",
    "        A.HorizontalFlip(p=0.5),  # Random horizontal flip with 50% probability\n",
    "        A.VerticalFlip(p=0.5),    # Random vertical flip with 50% probability\n",
    "        A.RandomRotate90(p=0.5),  # Random 90 degree rotation with 50% probability\n",
    "        ToTensorV2(), \n",
    "    ], is_check_shapes=True\n",
    ")\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "       A.Resize(512, 512),\n",
    "       ToTensorV2(),\n",
    "    ], is_check_shapes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Cloud_DrivenData_Dataset(\n",
    "    x_paths=train_x,\n",
    "    y_paths=train_y,\n",
    "    transform=training_transform, \n",
    "    bands = [\"B04\", \"B03\", \"B02\"]\n",
    ")\n",
    "valid_dataset = Cloud_DrivenData_Dataset(\n",
    "    x_paths=val_x,\n",
    "    y_paths=val_y,\n",
    "    transform=val_transform,\n",
    "    bands = [\"B04\", \"B03\", \"B02\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dl = DataLoader(train_dataset, batch_size=32, pin_memory=True, shuffle=True)\n",
    "val_dl = DataLoader(valid_dataset, batch_size=32, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test \n",
    "inputs = next(iter(train_dl))\n",
    "print(\"images shape : \" , inputs[\"image\"].shape)\n",
    "print(\"mask shape : \" , inputs[\"mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Test Model\n",
    "from models import ResNet_UNET\n",
    "import torch \n",
    "model = ResNet_UNET(in_channels=3,out_channels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = torch.randn((32, 3, 512, 512))\n",
    "    outputs = model.predict(inputs)\n",
    "    print(\"Predicted output shape \", outputs.shape)\n",
    "    outputs = model.forward(inputs)\n",
    "    print(\"Predicted output shape \", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from losses import DiceLoss\n",
    "from metrics import accuracy, f1_score, iou_score, recall, precision\n",
    "# Define functions, losses and metrics \n",
    "\n",
    "# Define Optimize \n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "optimizer = optim.AdamW\n",
    "params_opt = {\"lr\":lr, \"weight_decay\":weight_decay}\n",
    "# Define a Scheduler \n",
    "scheduler = optim.lr_scheduler.StepLR # Decreases LR by a factor of 0.1 every 10 epochs\n",
    "params_sc = {'step_size':10, 'gamma': 0.1}\n",
    "# Define Loss\n",
    "criterion = DiceLoss(mode=\"multiclass\")\n",
    "# Define Metrics \n",
    "metrics = [accuracy, f1_score, iou_score, recall, precision]\n",
    "\n",
    "# Early Stopping \n",
    "early_stopping_params = {\"patience\":5, \"trigger_times\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import train \n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    train_dl=train_dl,\n",
    "    valid_dl=val_dl,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    params_opt=params_opt,\n",
    "    params_sc=params_sc,\n",
    "    loss_fn=criterion,\n",
    "    metrics=metrics,\n",
    "    nb_epochs=50,\n",
    "    experiment_name=\"ResNet_Unet\",\n",
    "    log_dir=\"../runs\",\n",
    "    model_dir=\"../models\",\n",
    "    early_stopping_params=early_stopping_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Segformer on Cloud Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Test Model\n",
    "from models import Segformer\n",
    "from training import train \n",
    "\n",
    "model_name = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    "label2id = {\"cloud\": 1, \"no_cloud\": 0 }\n",
    "id2label = {v: k for k,v in label2id.items()}\n",
    "num_labels = 2\n",
    "\n",
    "segformer = Segformer(\n",
    "    model_name=model_name,\n",
    "    label2id=label2id,\n",
    "    num_labels=2,\n",
    "    freeze_encoder=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model parameter dtype:\", next(segformer.parameters()).dtype)\n",
    "print(\"Input tensor dtype:\", batch[\"image\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\n",
    "import torch.optim as optim\n",
    "# Assuming model is already defined and loaded\n",
    "# optimizer setup\n",
    "lr = 6e-5\n",
    "weight_decay = 0.01\n",
    "num_epochs = 50 \n",
    "total_steps = num_epochs * len(train_dl)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW\n",
    "params_opt = {\"lr\":lr, \"weight_decay\":weight_decay}\n",
    "\n",
    "# Warm-up and Cosine Annealing Scheduler\n",
    "warmup_steps = int(0.01 * total_steps)  # e.g., 1% of total steps as warm-up\n",
    "\n",
    "# Lambda function for warm-up\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return max(\n",
    "        0.0,\n",
    "        0.5 * (1.0 + torch.cos(torch.pi * (current_step - warmup_steps) / (total_steps - warmup_steps))),\n",
    "    )\n",
    "\n",
    "# LambdaLR with the custom lr_lambda\n",
    "scheduler = LambdaLR\n",
    "params_sc = {\"lr_lambda\":lr_lambda}\n",
    "\n",
    "# Define Loss\n",
    "criterion = DiceLoss(mode=\"multiclass\")\n",
    "# Define Metrics \n",
    "metrics = [accuracy, f1_score, iou_score, recall, precision]\n",
    "\n",
    "# Early Stopping \n",
    "early_stopping_params = {\"patience\":5, \"trigger_times\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    model=segformer,\n",
    "    train_dl=train_dl,\n",
    "    valid_dl=val_dl,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    params_opt=params_opt,\n",
    "    params_sc=params_sc,\n",
    "    loss_fn=criterion,\n",
    "    metrics=metrics,\n",
    "    nb_epochs=50,\n",
    "    experiment_name=\"Segformer_DrivenData\",\n",
    "    log_dir=\"../runs\",\n",
    "    model_dir=\"../models\",\n",
    "    early_stopping_params=early_stopping_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cloud Segmentation Model on Puerto Rico Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Puerto_Rico_Building_Dataset\n",
    "\n",
    "data_puerto = Puerto_Rico_Building_Dataset(\n",
    "    base_dir=\"../data/Puerto_Rico_dataset/tiff_tiles\",\n",
    "    pre_disaster_dir=\"Pre_Event_Grids_In_TIFF\",\n",
    "    post_disaster_dir=\"Post_Event_Grids_In_TIFF\",\n",
    "    mask_dir=\"Post_Event_Grids_In_TIFF_mask\",\n",
    "    transform=None,\n",
    "    extension=\"tif\",\n",
    "    cloud_filter_params=None,\n",
    "    preprocessing_mode=\"None\",\n",
    "    filtered_list_path=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puerto_loader = DataLoader(data_puerto, batch_size=32, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def display_predictions_batch(images, mask_predictions, mask_labels):\n",
    "    \"\"\"\n",
    "    Displays a batch of images alongside their predicted masks and ground truth masks.\n",
    "\n",
    "    Args:\n",
    "        images (torch.Tensor or numpy.ndarray): Batch of input images, shape (N, C, H, W) or (N, H, W, C).\n",
    "        mask_predictions (torch.Tensor or numpy.ndarray): Batch of predicted masks, shape (N, H, W) or (N, H, W, C).\n",
    "        mask_labels (torch.Tensor or numpy.ndarray): Batch of ground truth masks, shape (N, H, W) or (N, H, W, C).\n",
    "    \"\"\"\n",
    "    # Convert tensors to numpy arrays if needed\n",
    "    if isinstance(images, torch.Tensor):\n",
    "        images = images.detach().cpu().numpy()\n",
    "    if isinstance(mask_predictions, torch.Tensor):\n",
    "        mask_predictions = mask_predictions.detach().cpu().numpy()\n",
    "    if isinstance(mask_labels, torch.Tensor):\n",
    "        mask_labels = mask_labels.detach().cpu().numpy()\n",
    "    \n",
    "    batch_size = images.shape[0]  # Number of images in the batch\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        image = images[i]\n",
    "        mask_prediction = mask_predictions[i]\n",
    "        mask_label = mask_labels[i]\n",
    "        \n",
    "        # Handle grayscale or channel-first images\n",
    "        if image.ndim == 3 and image.shape[0] in [1, 3]:  # (C, H, W) format\n",
    "            image = np.transpose(image, (1, 2, 0))  # Convert to (H, W, C)\n",
    "        \n",
    "        # Normalize image for better visualization (if needed)\n",
    "        if image.max() > 1:\n",
    "            image = image / 255.0  # Assuming image is in [0, 255]\n",
    "        \n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Show the input image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Input Image\")\n",
    "        \n",
    "        # Show the predicted mask\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(mask_prediction, cmap='jet', interpolation='none')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Predicted Mask\")\n",
    "        \n",
    "        # Show the ground truth mask\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(mask_label, cmap='jet', interpolation='none')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Ground Truth Mask\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = next(iter(puerto_loader))\n",
    "images = inputs[\"pre_image\"].to(\"cuda\")\n",
    "outputs = segformer.predict(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_predictions_batch(images=images, mask_predictions=outputs, mask_labels=inputs[\"mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segformer.save(path=\"../models/Segformer_cloud_seg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
