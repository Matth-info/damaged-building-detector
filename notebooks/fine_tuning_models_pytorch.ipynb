{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fined tuning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal : From the dataset containing Post Event and Pre Event images with segmentation masks from MS building footprint dataset, we want to build different useful Computer Vision models like a Classifier, an Object detector and an Instance Segmentation model using different techniques like Transfer learning. A labelising work is required to build stronger model on different task. We propose to use Active Learning to find the most relevant items and Conformal prediction on Classification and Segmentation task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some stats about the data : \n",
    "- more than 10 000 tiles from the raw high quality tiff images. \n",
    "\n",
    "Most useful files :\n",
    "- damage_detection/filtered_jpeg_tiles/Post_Event_Grids_In_jpeg\n",
    "- damage_detection/filtered_jpeg_tiles/Post_Event_Grids_mask_jpeg \n",
    "- damage_detection/filtered_jpeg_tiles/Pre_Event_Grids_In_jpeg\n",
    "\n",
    "We consider the masks are the same for Pre and Post Event images.\n",
    "\n",
    "To load the data from s3: \n",
    "mc cp s3/mbesnier/diffusion/damage_detection/filtered_jpeg_tiles/<folder_name> . --recursive \n",
    "\n",
    "mc cp s3/mbesnier/diffusion/damage_detection/filtered_jpeg_tiles/Post_Event_Grids_In_jpeg . --recursive \\\n",
    "mc cp s3/mbesnier/diffusion/damage_detection/filtered_jpeg_tiles/Pre_Event_Grids_In_jpeg . --recursive \\\n",
    "mc cp s3/mbesnier/diffusion/damage_detection/filtered_jpeg_tiles/Post_Event_Grids_mask_jpeg . --recursive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GeoTiff Images\n",
    "import rasterio\n",
    "\n",
    "# Visualisation\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from matplotlib.pyplot import figure\n",
    "from PIL import Image\n",
    "\n",
    "# Others\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "from tqdm import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the geotiff file\n",
    "import re \n",
    "def extract_title_ij(filename):\n",
    "    pattern_base = r\"(\\w+)/tile_(\\d+)_(\\d+).jpg\"\n",
    "    pattern_mask = r\"(\\w+)/tile_(\\d+)_(\\d+)_mask.jpg\"\n",
    "    match_base = re.search(pattern_base, filename)\n",
    "    match_mask = re.search(pattern_mask, filename)\n",
    "    if match_base:\n",
    "        title , i, j = map(str, match_base.groups())\n",
    "        return title, i, j\n",
    "    if match_mask:\n",
    "        title , i, j = map(str, match_mask.groups())\n",
    "        return title, i, j\n",
    "    return None, None, None\n",
    "\n",
    "def load_and_visualize(image_path):\n",
    "    \"\"\"\n",
    "    display a tif image (little tiles are recommended)\n",
    "    \"\"\"\n",
    "    with rasterio.open(image_path) as src:\n",
    "        # Read the red, green, and blue bands directly into a 3D array\n",
    "        image_rgb = src.read([1, 2, 3])  # Read bands 1, 2, and 3 in a single call\n",
    "        # Transpose the array for correct display (optional)\n",
    "        image_rgb = image_rgb.transpose(1, 2, 0)\n",
    "\n",
    "        title , i, j = extract_title_ij(image_path)\n",
    "\n",
    "        # Display the RGB composite image\n",
    "        plt.imshow(image_rgb)\n",
    "        plt.title(f\"Image from {title} at position (i={i},j={j})\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 36\n",
    "j = 109\n",
    "\n",
    "load_and_visualize(f\"./data/Post_Event_Grids_In_jpeg/tile_{i}_{j}.jpg\")\n",
    "load_and_visualize(f\"./data/Pre_Event_Grids_In_jpeg/tile_{i}_{j}.jpg\")\n",
    "#load_and_visualize(f\"Post_Event_Grids_mask_jpeg/tile_{i}_{j}_mask.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Set Hyperparameters \n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "# Define the transformation (resize, normalize, etc.) that you want to apply to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),    # Resize the images to 224x224 pixels\n",
    "    transforms.ToTensor(),            # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization with ImageNet mean/std\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "full_dataset = datasets.ImageFolder(root=\"./data\", transform=transform)\n",
    "\n",
    "# Define a dictionary to map the original class names to new ones\n",
    "# Format: {original_class_name: new_class_name}\n",
    "class_rename_map = {\n",
    "    'Post_Event_Grids_In_jpeg': 'Post_disaster',\n",
    "    'Pre_Event_Grids_In_jpeg': 'Pre_disaster'\n",
    "}\n",
    "# The new mapping of the class indices based on the renaming\n",
    "new_class_names = [class_rename_map[cls] for cls in full_dataset.classes]\n",
    "full_dataset.classes = new_class_names\n",
    "class_to_idx = {new_name: full_dataset.class_to_idx[old_name] for old_name, new_name in class_rename_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation splits\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_ds, val_ds = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Example of iterating through one batch of the training data\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch size: {images.size()}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    break  # Remove this break to go through the entire dataset\n",
    "\n",
    "# Checking class names\n",
    "class_names = full_dataset.classes\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "# Optionally, get the size of the datasets\n",
    "print(f\"Training samples: {len(train_ds)}\")\n",
    "print(f\"Validation samples: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter  # Import TensorBoard writer\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "\n",
    "# Define device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a pretrained model (e.g., ResNet) and modify for your task\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the final fully connected layer to match the number of classes\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(full_dataset.classes)) # add a FCN as a head of Pre-trained model\n",
    "\n",
    "# Move the model to the device (GPU or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set up a SummaryWriter \n",
    "writer = SummaryWriter('runs/experiment1')  # Create a folder to log everything\n",
    "\n",
    "# Optionally, log the model graph to TensorBoard\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Example input for model graph\n",
    "writer.add_graph(model, dummy_input)\n",
    "\n",
    "# 7. Training function\n",
    "def train(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Log the training loss every 10 batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            writer.add_scalar('Training Loss', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    # Log validation loss and accuracy\n",
    "    writer.add_scalar('Validation Loss', epoch_loss, epoch)\n",
    "    writer.add_scalar('Validation Accuracy', epoch_acc, epoch)\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device, epoch)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device, epoch)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save model if it has the best accuracy so far\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "# 11. Close TensorBoard writer\n",
    "writer.close()\n",
    "\n",
    "# Print total training time\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Training complete in {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model,(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "def unnormalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    img = img * std + mean  # Unnormalize\n",
    "    return img\n",
    "\n",
    "\n",
    "# Function to show images (helper function)\n",
    "def imshow(img, title=None):\n",
    "    img = unnormalize(img)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Show images in a grid\n",
    "def show_image_grid(images, labels, preds, class_names, title):\n",
    "    images = [unnormalize(img) for img in images]\n",
    "    images = make_grid(images, nrow=3)\n",
    "    npimg = images.numpy()\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to visualize model performance in a grid\n",
    "def visualize_model_performance_grid(model, val_loader, device, class_names, num_images=6):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    correct_images = []\n",
    "    correct_labels = []\n",
    "    correct_preds = []\n",
    "    incorrect_images = []\n",
    "    incorrect_labels = []\n",
    "    incorrect_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # Collect correct predictions\n",
    "            for i in range(inputs.size(0)):\n",
    "                if preds[i] == labels[i] and len(correct_images) < num_images // 2:\n",
    "                    correct_images.append(inputs[i].cpu())\n",
    "                    correct_labels.append(labels[i].cpu())\n",
    "                    correct_preds.append(preds[i].cpu())\n",
    "                \n",
    "                # Collect incorrect predictions\n",
    "                elif preds[i] != labels[i] and len(incorrect_images) < num_images // 2:\n",
    "                    incorrect_images.append(inputs[i].cpu())\n",
    "                    incorrect_labels.append(labels[i].cpu())\n",
    "                    incorrect_preds.append(preds[i].cpu())\n",
    "\n",
    "            # Stop when we have enough correct and incorrect images\n",
    "            if len(correct_images) >= num_images // 2 and len(incorrect_images) >= num_images // 2:\n",
    "                break\n",
    "\n",
    "    # Show correct predictions in a grid\n",
    "    if correct_images:\n",
    "        show_image_grid(\n",
    "            torch.stack(correct_images), \n",
    "            correct_labels, \n",
    "            correct_preds, \n",
    "            class_names, \n",
    "            title=\"Correct Predictions\"\n",
    "        )\n",
    "\n",
    "    # Show incorrect predictions in a grid\n",
    "    if incorrect_images:\n",
    "        show_image_grid(\n",
    "            torch.stack(incorrect_images), \n",
    "            incorrect_labels, \n",
    "            incorrect_preds, \n",
    "            class_names, \n",
    "            title=\"Incorrect Predictions\"\n",
    "        )\n",
    "\n",
    "# Example usage:\n",
    "visualize_model_performance_grid(model, val_loader, device, full_dataset.classes, num_images=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
