{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import (\n",
    "    Puerto_Rico_Building_Dataset, \n",
    "    Cloud_DrivenData_Dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_DIR = Path.cwd().parent.resolve() / \"data/data_samples/Cloud_DrivenData/final/public\"\n",
    "TRAIN_FEATURES = DATA_DIR / \"train_features\"\n",
    "TRAIN_LABELS = DATA_DIR / \"train_labels\"\n",
    "TRAIN_META_FILE = DATA_DIR / \"train_metadata.csv\"\n",
    "BANDS = [\"B04\", \"B03\", \"B02\"] #\"B08\"\n",
    "assert TRAIN_FEATURES.exists()\n",
    "assert TRAIN_META_FILE.exists()\n",
    "\n",
    "import pandas as pd\n",
    "train_meta = pd.read_csv(TRAIN_META_FILE)\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def add_paths(df: pd.DataFrame, feature_dir: Path, label_dir: Path = None, bands: list = BANDS) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds file paths for each band and label to the dataframe based on chip_id.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing chip_id (e.g., image identifiers).\n",
    "        feature_dir (Path): Directory where feature TIF images are stored.\n",
    "        label_dir (Path, optional): Directory where label TIF images are stored. Defaults to None.\n",
    "        bands (list): List of band names (e.g., [\"B02\", \"B03\", ...]). Defaults to BANDS.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated dataframe with new columns for each band path and label path.\n",
    "    \n",
    "    Adds the following columns to the dataframe:\n",
    "        - \"{band}_path\" for each band image.\n",
    "        - \"label_path\" for the label image, if `label_dir` is provided.\n",
    "        - \"has_{band}_path\" boolean column indicating if the feature file exists.\n",
    "        - \"has_image_channels\" boolean column indicating if all feature band files exist.\n",
    "        - \"has_label_path\" boolean column indicating if the label file exists (if `label_dir` is provided).\n",
    "        - \"accessible\" boolean column indicating if all image channels and label file exist.\n",
    "    \"\"\"\n",
    "    # Ensure feature_dir and label_dir are Path objects\n",
    "    feature_dir = Path(feature_dir)\n",
    "    if label_dir is not None:\n",
    "        label_dir = Path(label_dir)\n",
    "\n",
    "    selected_columns = [\"chip_id\", \"location\", \"datetime\", \n",
    "                        \"cloudpath\"\n",
    "                        ]\n",
    "    \n",
    "    # Initialize columns to track file existence for each band\n",
    "    for band in bands:\n",
    "        df[f\"{band}_path\"] = feature_dir / df[\"chip_id\"] / f\"{band}.tif\"\n",
    "        # Check if the band file exists and add a boolean column\n",
    "        df[f\"has_{band}_path\"] = df[f\"{band}_path\"].apply(lambda x: x.exists())\n",
    "        selected_columns.append(f\"{band}_path\")\n",
    "\n",
    "    # Add \"has_image_channels\" to check if all bands exist\n",
    "    df[\"has_image_channels\"] = df[[f\"has_{band}_path\" for band in bands]].all(axis=1)\n",
    "    # Add label path and check existence if label_dir is provided\n",
    "    if label_dir is not None:\n",
    "        df[\"label_path\"] = label_dir / (df[\"chip_id\"] + \".tif\")   \n",
    "        # Check if the label file exists and add a boolean column\n",
    "        df[\"has_label_path\"] = df[\"label_path\"].apply(lambda x: x.exists())\n",
    "        selected_columns.append(\"label_path\")\n",
    "    \n",
    "    # Add \"accessible\" column to check if all bands and label file exist\n",
    "    df[\"accessible\"] = df[\"has_image_channels\"] & df[\"has_label_path\"]\n",
    "    \n",
    "    return df[df[\"accessible\"] == True][selected_columns]\n",
    "\n",
    "train_meta = add_paths(train_meta, TRAIN_FEATURES, TRAIN_LABELS)\n",
    "\n",
    "#################\n",
    "import random\n",
    "random.seed(9)  # set a seed for reproducibility\n",
    "\n",
    "# put 1/3 of chips into the validation set\n",
    "chip_ids = train_meta.chip_id.unique().tolist()\n",
    "val_chip_ids = random.sample(chip_ids, round(len(chip_ids) * 0.2))\n",
    "\n",
    "val_mask = train_meta.chip_id.isin(val_chip_ids)\n",
    "val = train_meta[val_mask].copy().reset_index(drop=True)\n",
    "train = train_meta[~val_mask].copy().reset_index(drop=True)\n",
    "\n",
    "val.shape, train.shape\n",
    "#################\n",
    "# separate features from labels\n",
    "feature_cols = [\"chip_id\"] + [f\"{band}_path\" for band in BANDS]\n",
    "\n",
    "train_x = train[feature_cols].copy()\n",
    "train_y = train[[\"chip_id\", \"label_path\"]].copy()\n",
    "\n",
    "val_x = val[feature_cols].copy()\n",
    "val_y = val[[\"chip_id\", \"label_path\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Augmentation (Tested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import albumentations.augmentations.functional as F\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_transform = A.Compose(\n",
    "    [\n",
    "        A.HorizontalFlip(p=0.5),  # Random horizontal flip with 50% probability\n",
    "        A.VerticalFlip(p=0.5),    # Random vertical flip with 50% probability\n",
    "        A.RandomRotate90(p=0.5),  # Random 90 degree rotation with 50% probability\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5, border_mode=0\n",
    "        ),  # Random shift, scale, and rotation with fill at borders\n",
    "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),  # Adjust color properties\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),  # Adjust brightness and contrast\n",
    "        A.GaussianBlur(blur_limit=(3, 5), p=0.3),  # Apply a Gaussian blur\n",
    "        A.GaussNoise(var_limit=(10, 50), p=0.3),  # Add Gaussian noise\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize to standard values\n",
    "        ToTensorV2(),  # Convert to PyTorch tensors\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Cloud_DrivenData_Dataset(\n",
    "        x_paths= train_x,\n",
    "        bands = BANDS,\n",
    "        y_paths = train_y,\n",
    "        transform = segmentation_transform,\n",
    "        pytorch=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "\n",
    "def show_images(original : np.array, augmented : np.array):\n",
    "    \"\"\"Display the original and augmented images.\"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(original)\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[1].imshow(augmented)\n",
    "    axs[1].set_title(\"Augmented Image\")\n",
    "\n",
    "def show_mask(original : np.array, augmented : np.array):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(original)\n",
    "    axs[0].set_title(\"Original Mask\")\n",
    "    axs[1].imshow(augmented)\n",
    "    axs[1].set_title(\"Augmented Mask\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_puerto = Puerto_Rico_Building_Dataset(\n",
    "    base_dir=\"../data/data_samples/Puerto_Rico_dataset/\",\n",
    "    pre_disaster_dir=\"Pre_Event_Grids_In_TIFF\",\n",
    "    post_disaster_dir=\"Post_Event_Grids_In_TIFF\",\n",
    "    mask_dir=\"Grids_In_TIFF_mask\",\n",
    "    transform=segmentation_transform,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data_puerto[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
